---
title: "Main"
author: "Tan Hao Qin"
date: "2 August 2016"
output: pdf_document
---

```{r setup, include=FALSE, error=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library('mlogit')
```

## Read data, always do this
```{r utility functions}
prediction_score <-function (actual, p){
  logloss <- 0
  N <- length(actual)
  for(i in 1:N){
    for(j in 1:4){
      y <- ifelse(as.integer(actual[i]) == j, 1, 0)
      p_score <- p[i,j]
      l <- log(as.numeric(p_score))
      logloss <- logloss + -1/N*y*l
    }
  }
  logloss
}

normalize_data <- function(D){
  D[,21:40] <- scale(D[,21:40])
  D[,5:7] <- scale(D[,5:7])
  D[,9:10] <- scale(D[,9:10])
  D
}
```

```{r read csv}
train <- read.csv('train.csv')
test <- read.csv('test.csv')
test2 <- read.csv('test.csv')
submission <- read.csv('samplesubmission.csv')
str(train)
str(test)
summary(train)
summary(test)
```


```{r format test data}
test$income <- lapply(test$income, as.character)
test$income <- factor(test$income,levels=levels(train$income))

Test <- mlogit.data(test, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
Test$Ch1 = 0
Test$Ch2 = 0
Test$Ch3 = 0
Test$Ch4 = 0
Test$Choice = FALSE
```


```{r format train data with factors to numbers}
train2 <- read.csv('train.csv')
summary(train2)
str(train2)

#change gender, male = 1, female = 0
train2$gender <- ifelse(train2$gender == 'Male', 1, 0)


#change miles to avg of range, except over 400 which becomes 500
train2$miles <- factor(train2$miles, levels = c("Under 50 Miles","51 To 100 Miles","101 To 150 Miles","151 To 200 Miles","201 To 250 Miles","251 To 300 Miles","301 To 350 Miles","351 To 400 Miles","Over 400 Miles"))
table(train2$miles)
train2$miles <- (as.numeric(train2$miles)-1)*50 + 25
table(train2$miles)
train2$miles <- ifelse(train2$miles == 425, 500, train2$miles)
table(train2$miles)

#change night percentage to average of range
train2$night<- factor(train2$night, levels = c("Under 10%","10% To 20%","21% To 30%", "31% To 40%", "41% To 50%","51% To 60%","61% To 70%", "71% To 80%","81% To 90%","91% To 100%" ))
table(train2$night)
train2$night <- (as.numeric(train2$night)-1)*10+5
table(train2$night)

#change age to average of age, except over 60 changed to 70
train2$age <- factor(train2$age, levels = c("Under 30", "30 To 39", "40 To 49", "50 To 59", "60 & Over"))
table(train2$age)
train2$age <- (as.numeric(train2$age)-1)*10+25
train2$age <- ifelse(train2$age==65, 70, train2$age)

str(train2)


#change urbanisation, with Rural/Country = 0, Suburban = 1, Urban/City = 2
train2$Urb <- ifelse(train2$Urb=="Rural/Country", 0, ifelse(train2$Urb=="Suburban", 1, ifelse(train2$Urb=="Urban/City", 2, train2$Urb)))
str(train2$Urb)
str(train2)


#change education, Grade School = 0, High School = 1, Trade/Vocational School = 2, Some College (1-3 Years) = 3, College Graduate (4 Years) = 4, Postgraduate College = 5



train2$educ <- as.character(train2$educ)

train2$educ <- factor(train2$educ, levels = c("Grade School","High School","Trade/Vocational School","Some College (1-3 Years)","College Graduate (4 Years)","Postgraduate College"))
table(train2$educ)
train2$educ <- as.numeric(train2$educ)

str(train2$educ)

str(train2)

# change income to average of range
summary(train2$income)

train2$income <- factor(train2$income, levels = c("Under $29,999", "$30,000 to $39,999", "$40,000 to $49,999", "$50,000 to $59,999", "$60,000 to $69,999", "$70,000 to $79,999", "$80,000 to $89,999", "$90,000 to $99,999","$100,000 to $109,999", "$110,000 to $119,999", "$120,000 to $129,999", "$130,000 to $139,999", "$140,000 to $149,999", "$150,000 to $159,999", "$160,000 to $169,999", "$170,000 to $179,999", "$180,000 to $189,999", "$190,000 to $199,999", "$200,000 to $209,999", "$210,000 to $219,999", "$220,000 to $229,999","$230,000 to $239,999", "$240,000 to $249,999", "$250,000 to $259,999", "$260,000 to $269,999", "$270,000 to $279,999", "$280,000 to 2$89,999", "$290,000 to $299,999", "$300,000 & Over"))

table(train2$income)
train2$income <- (as.numeric(train$income)-1)*10000+25000
train2$income <- ifelse(train2$income==25000, 20000, ifelse(train2$income==245000, 250000, train2$income))

str(train2)

#change parking frequency to number of times per year
summary(train2$ppark)
head(train2$ppark)

train2$ppark <- as.character(train2$ppark)
train2$ppark <- factor(train2$ppark, levels = c("Never", "Yearly", "Monthly", "Weekly",  "Daily"))
train2$ppark <- ifelse(train2$ppark == "Never", 0, ifelse(train2$ppark =="Yearly", 1, ifelse(train2$ppark == "Monthly", 12,ifelse(train2$ppark == "Weekly", 52, ifelse(train2$ppark == "Daily", 365, train2$ppark)))))

summary(train2$ppark)

#ignore region, segment of car

```

```{r format test with numbers for factors}
test2$income <- lapply(test2$income, as.character)
test2$income <- factor(test2$income, levels=levels(train$income))

#change gender, male = 1, female = 0
test2$gender <- ifelse(test2$gender == 'Male', 1, 0)


#change miles to avg of range, except over 400 which becomes 500
test2$miles <- factor(test2$miles, levels = c("Under 50 Miles","51 To 100 Miles","101 To 150 Miles","151 To 200 Miles","201 To 250 Miles","251 To 300 Miles","301 To 350 Miles","351 To 400 Miles","Over 400 Miles"))
table(test2$miles)
test2$miles <- (as.numeric(test2$miles)-1)*50 + 25
table(test2$miles)
test2$miles <- ifelse(test2$miles == 425, 500, test2$miles)
table(test2$miles)

#change night percentage to average of range
test2$night<- factor(test2$night, levels = c("Under 10%","10% To 20%","21% To 30%", "31% To 40%", "41% To 50%","51% To 60%","61% To 70%", "71% To 80%","81% To 90%","91% To 100%" ))
table(test2$night)
test2$night <- (as.numeric(test2$night)-1)*10+5
table(test2$night)

#change age to average of age, except over 60 changed to 70
test2$age <- factor(test2$age, levels = c("Under 30", "30 To 39", "40 To 49", "50 To 59", "60 & Over"))
table(test2$age)
test2$age <- (as.numeric(test2$age)-1)*10+25
test2$age <- ifelse(test2$age==65, 70, test2$age)

str(test2)

#change urbanisation, with Rural/Country = 0, Suburban = 1, Urban/City = 2
test2$Urb <- ifelse(test2$Urb=="Rural/Country", 0, ifelse(test2$Urb=="Suburban", 1, ifelse(test2$Urb=="Urban/City", 2, test2$Urb)))
str(test2$Urb)
str(test2)


#change education, Grade School = 0, High School = 1, Trade/Vocational School = 2, Some College (1-3 Years) = 3, College Graduate (4 Years) = 4, Postgraduate College = 5

test2$educ <- as.character(test2$educ)

test2$educ <- factor(test2$educ, levels = c("Grade School","High School","Trade/Vocational School","Some College (1-3 Years)","College Graduate (4 Years)","Postgraduate College"))
table(test2$educ)
test2$educ <- as.numeric(test2$educ)

str(test2)

# change income to average of range
summary(test2$income)


test2$income <- factor(test2$income, levels = c("Under $29,999", "$30,000 to $39,999", "$40,000 to $49,999", "$50,000 to $59,999", "$60,000 to $69,999", "$70,000 to $79,999", "$80,000 to $89,999", "$90,000 to $99,999","$100,000 to $109,999", "$110,000 to $119,999", "$120,000 to $129,999", "$130,000 to $139,999", "$140,000 to $149,999", "$150,000 to $159,999", "$160,000 to $169,999", "$170,000 to $179,999", "$180,000 to $189,999", "$190,000 to $199,999", "$200,000 to $209,999", "$210,000 to $219,999", "$220,000 to $229,999","$230,000 to $239,999", "$240,000 to $249,999", "$250,000 to $259,999", "$260,000 to $269,999", "$270,000 to $279,999", "$280,000 to 2$89,999", "$290,000 to $299,999", "$300,000 & Over"))

table(test2$income)
test2$income <- (as.numeric(test$income)-1)*10000+25000
test2$income <- ifelse(test2$income==25000, 20000, ifelse(test2$income==245000, 250000, test2$income))
test2$income[is.na(test2$income)] <- mean(test2$income,na.rm=T)

str(test2)

#change parking frequency to number of times per year
summary(test2$ppark)
head(test2$ppark)

test2$ppark <- as.character(test2$ppark)
test2$ppark <- factor(test2$ppark, levels = c("Never", "Yearly", "Monthly", "Weekly",  "Daily"))
test2$ppark <- ifelse(test2$ppark == "Never", 0, ifelse(test2$ppark =="Yearly", 1, ifelse(test2$ppark == "Monthly", 12,ifelse(test2$ppark == "Weekly", 52, ifelse(test2$ppark == "Daily", 365, test2$ppark)))))

summary(test2$ppark)

#ignore region, segment of car


test2 <- mlogit.data(test2, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
test2$Ch1 = 0
test2$Ch2 = 0
test2$Ch3 = 0
test2$Ch4 = 0
test2$Choice = FALSE

#install.packages("Amelia")
library(Amelia)
missmap(test2, main = "Missing values vs observed")

# all values are here
```

Tasks <= 12 are in D and tasks > 12 are in V
shape is wide because 4 choices are in the same row
```{r seperate train data for testing}
A <- mlogit.data(train, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

D <- mlogit.data(subset(train, Task <= 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

V <- mlogit.data(subset(train, Task > 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
ActualChoice <- subset(train, Task > 12)[,"Choice"]
```

```{r seperate train2 data for testing with converted factors}
A2 <- mlogit.data(train2, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

D2 <- mlogit.data(subset(train2, Task <= 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

V2 <- mlogit.data(subset(train2, Task > 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
ActualChoice2 <- subset(train2, Task > 12)[,"Choice"]
```

```{r generating data for proper cross validation}

start <- c(1, 4, 7, 10, 13, 16)
stop <- c(4, 7, 10, 13, 16, 19)

actualVector <- vector(mode = "list", length = 6)
trainVector <- vector(mode = "list", length = 6)
validateVector <- vector(mode = "list", length = 6)

for (i in 1:6){
  trainVector[[i]] <-mlogit.data(subset(train2, Task >= start[i]&Task < stop[i]), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")   
  validateVector[[i]] <-mlogit.data(subset(train2, !(Task >= start[i]&Task < stop[i])), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
    actualVector[[i]] <-subset(train2, !(Task >= start[i]&Task < stop[i]))[,'Choice']
  #normalize_data(trainVector[[i]])
  #normalize_data(validateVector[[i]])
}

```

```{r cv}

coef1 <- NA
coef2 <- NA
coef3 <- NA
coef4 <- NA

cvmodel1Vector <- vector(mode = "list", length = 6)
cvmodel2Vector <- vector(mode = "list", length = 6)
cvmodel3Vector <- vector(mode = "list", length = 6)
cvmodel4Vector <- vector(mode = "list", length = 6)
score1 <- c()
score2 <- c()
score3 <- c()
score4 <- c()

for (i in 1:6){
  cvmodel1Vector[[i]] <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+miles+night+age+Urb, data = trainVector[[i]])

  cvmodel2Vector[[i]] <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+age+Urb, data = trainVector[[i]], reflevel = "Ch4")
  
  cvmodel3Vector[[i]] <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+age+Urb, data = trainVector[[i]])
  
  cvmodel4Vector[[i]] <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+age+Urb, data = trainVector[[i]])
  
  predictmodel1 <- predict(cvmodel1Vector[[i]], newdata = validateVector[[i]])
  predictmodel2 <- predict(cvmodel2Vector[[i]], newdata = validateVector[[i]])
  predictmodel3 <- predict(cvmodel3Vector[[i]], newdata = validateVector[[i]])
  predictmodel4 <- predict(cvmodel4Vector[[i]], newdata = validateVector[[i]])
  
  score1 <- c(score1, prediction_score(actualVector[[i]], predictmodel1))
  score2 <- c(score2, prediction_score(actualVector[[i]], predictmodel2))
  score3 <- c(score3, prediction_score(actualVector[[i]], predictmodel3))
  score4 <- c(score4, prediction_score(actualVector[[i]], predictmodel4))
  
  if(is.na(coef1)){
    coef1 <- summary(cvmodel1Vector[[i]])$CoefTable[,4]
    coef2 <- summary(cvmodel2Vector[[i]])$CoefTable[,4]
    coef3 <- summary(cvmodel3Vector[[i]])$CoefTable[,4]
    coef4 <- summary(cvmodel4Vector[[i]])$CoefTable[,4]
  } else {
    coef1 <- pmin(summary(cvmodel1Vector[[i]])$CoefTable[,4],coef1)
    coef2 <- pmin(summary(cvmodel2Vector[[i]])$CoefTable[,4],coef2)
    coef3 <- pmin(summary(cvmodel3Vector[[i]])$CoefTable[,4],coef3)
    coef4 <- pmin(summary(cvmodel4Vector[[i]])$CoefTable[,4],coef4)
  }
}

#model1
mean(score1)
#model2
mean(score2)
#model3
mean(score3)
mean(score4)
```

##Simple models
```{r model1 mlogit}
model1 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = D)
summary(model1)

model3star <- mlogit(Choice~CC+BU+KA+SC+TS+MA+Price-1, data = D)
summary(model3star)

model2star <- mlogit(Choice~CC+BU+KA+SC+TS+MA+GN+LD+Price-1, data = D)
summary(model2star)

model1star <- mlogit(Choice~CC+BU+KA+SC+TS+MA+GN+LD+NS+BZ+RP+LB+HU+Price-1, data = D)
summary(model1star)
```

```{r validate}
predict1 <- predict(model1, newdata = V)
predict3star <- predict(model3star, newdata = V)
predict2star <- predict(model2star, newdata = V)
predict1star <- predict(model1star, newdata = V)

PredictChoice1 <- apply(predict1, 1, which.max)
PredictChoice3star <- apply(predict3star, 1, which.max)
PredictChoice2star <- apply(predict2star, 1, which.max)
PredictChoice1star <- apply(predict1star, 1, which.max)

t1 <- table(ActualChoice, PredictChoice1)
t3s <- table(ActualChoice, PredictChoice3star)
t2s <- table(ActualChoice, PredictChoice2star)
t1s <- table(ActualChoice, PredictChoice1star)

choiceVector <- vector(mode = "list", length = 4)
choiceVector[[1]] <- t1
choiceVector[[2]] <- t3s
choiceVector[[3]] <- t2s
choiceVector[[4]] <- t1s
```

model 1 with all variables is best
```{r compare accuracy}
accurate_cnt <- c()
for(i in 1:4){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + choiceVector[[i]][j,j]
  }
}
accurate_cnt
```



```{r pred1 mlogit all variables D data}
TestPredict <- predict(model1, newdata = Test)
Submission1 <- submission
for (i in 1:length(TestPredict)){
  for (j in colnames(TestPredict)){
    Submission1[i,j] = TestPredict[i,j]
  }
}
write.csv(Submission1, file = 'Submission1.csv')
```

```{r pred3 2nd best mlogit with 1star variables}
TestPredict <- predict(model1star, newdata = Test)
Submission3 <- submission
for (i in 1:length(TestPredict)){
  for (j in colnames(TestPredict)){
    Submission3[i,j] = TestPredict[i,j]
  }
}
write.csv(Submission3, file = 'Submission3.csv')
```

```{r model2 random parameters}
model2 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = D, rpar = c(CC = 'n',  GN = 'n',  NS = 'n',  BU = 'n',  FA = 'n',  LD = 'n',  BZ = 'n',  FC = 'n',  FP = 'n',  RP = 'n',  PP = 'n',  KA = 'n',  SC = 'n',  TS = 'n',  NV = 'n',  MA = 'n',  LB = 'n',  AF = 'n',  HU = 'n', Price = 'n'), panel = TRUE, print.level = TRUE)
summary(model2)
```

```{r pred2 random parameter}
Pred2 <- predict(model2, newdata = Test)
Submission2 <- submission
for (i in 1:length(Pred2)){
  for (j in colnames(Pred2)){
    Submission2[i,j] = Pred2[i,j]
  }
}
write.csv(Submission2, file = 'Submission2.csv')
```



```{r pred4 mlogit with A data}
A <- mlogit.data(train, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

model1A <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = A)
summary(model1A)

Pred1A <- predict(model1A, newdata = Test)
Submission4 <- submission
for (i in 1:nrow(Pred1A)){
  for (j in colnames(Pred1A)){
    Submission4[i,j] = Pred1A[i,j]
  }
}
write.csv(Submission4, file = 'Submission4.csv')
```

```{r pred5&6 mlogit with numerical factor data}

A <- mlogit.data(train, shape = 'wide', choice = "Choice", varying = c(c(4:83)), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")

model1A <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1 | gender+miles+night+age+Urb, data = A)
summary(model1A)

Pred1A <- predict(model1A, newdata = Test)
Submission6 <- submission
for (i in 1:nrow(Pred1A)){
  for (j in colnames(Pred1A)){
    Submission6[i,j] = Pred1A[i,j]
  }
}
write.csv(Submission6, file = 'Submission6.csv')

```

```{r model 7 with all factors}
model7_pre <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+miles+night+age+Urb, data = D2)
summary(model7_pre)


model7 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+miles+night+age+Urb+educ+income+ppark, data = D2)
summary(model7)

model7_3star <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1 | age+Urb, data = D2)
summary(model7_3star)

model7_2star <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1 | age+Urb+ppark, data = D2)
summary(model7_2star)

model7_1star <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1 | miles+age+Urb+educ+ppark, data = D2)
summary(model7_1star)

Pred7_pre <- predict(model7_pre, newdata = V2)
Pred7 <- predict(model7, newdata = V2)
Pred7_3star <- predict(model7_3star, newdata = V2)
Pred7_2star <- predict(model7_2star, newdata = V2)
Pred7_1star <- predict(model7_1star, newdata = V2)
```

```{r model 8 using glmnet}

library(glmnet)
newA2 <- A2[,4:41]
colnames(newA2)
x <- model.matrix(Choice~., newA2)
y <- newA2$Choice

grid <- 10^seq(10,-6,length=100)

set.seed(1)
glmnet_train<- sample(1:nrow(x), nrow(x)/2)
glmnet_test<- -glmnet_train

glm_model8<- glmnet(x[glmnet_train,], y[glmnet_train], family = 'multinomial', alpha=1, lambda = grid)
plot(glm_model8, xvar="lambda")

##BEST LAMBDA IS $lambda.min
set.seed(2)
cvlasso<- cv.glmnet(x[glmnet_train,], y[glmnet_train], alpha=1, family = 'multinomial')
cvlasso
#using D2 data, best lambda is 0.001681765
#gender+miles+night+age+Urb
#repeat above using D Data instead
#using D data, best lambda is 0.003885096
#using A data, best lambda is 0.002757943
#using A2 data, 0.001732069
#using newA2 data, 0.001732069
#using 2/3 of data for training

predictlasso<- predict(glm_model8, newx = x[glmnet_test,], s=cvlasso$lambda.min) #lambdaD2=0.001681765, lambdaD=0.003885096 lambdaA = 0.002757943 #lambdaA2 = 0.001732069 #lamdawith2/3 training set = 0.00131865
mean((predictlasso-y[glmnet_test])^2)
#mean squared error D2 is 0.1769509
#mean squared error D is 0.1770686
#mean squared error A is 0.1737358
#mean squared error A2 is 0.1736695
#mse 2/3 training set is 0.1720781


lassocoef <- predict(glm_model8, s = cvlasso$lambda.min, type ="coefficients")
?predict.glm
lassocoef
```
setwd("C:/Users/Susiwati/Dropbox/Susi/SUTD/40.220 The Analytics Edge/R/Susi/sutd-kaggle")


```{r validate model7}
PredictChoice7_pre <- apply(Pred7_pre, 1, which.max)
PredictChoice7 <- apply(Pred7, 1, which.max)
PredictChoice7_3star <- apply(Pred7_3star, 1, which.max)
PredictChoice7_2star <- apply(Pred7_2star, 1, which.max)
PredictChoice7_1star <- apply(Pred7_1star, 1, which.max)


t1_pre <- table(ActualChoice2, PredictChoice7_pre)
t1 <- table(ActualChoice2, PredictChoice7)
t3s <- table(ActualChoice2, PredictChoice7_3star)
t2s <- table(ActualChoice2, PredictChoice7_2star)
t1s <- table(ActualChoice2, PredictChoice7_1star)


choiceVector <- vector(mode = "list", length = 5)
choiceVector[[1]] <- t1_pre
choiceVector[[2]] <- t1
choiceVector[[3]] <- t3s
choiceVector[[4]] <- t2s
choiceVector[[5]] <- t1s

prediction_score(ActualChoice2,Pred7_pre)
prediction_score(ActualChoice2,Pred7)
prediction_score(ActualChoice2,Pred7_3star)
prediction_score(ActualChoice2,Pred7_2star)
prediction_score(ActualChoice2,Pred7_1star)

```

```{r validate model 8}
set.seed(1)

model8 <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price - 1|year+region+segment, data = D2)

#model8 <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+HU+Price - 1|miles+region+segment+income, data = D2)

#model8 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1 |educ+night+ppark+region+segment, data = D2)
#took out educ, income
summary(model8)

Pred8 <- predict(model8, newdata = V2)

PredictChoice8 <- apply(Pred8, 1, which.max)

prediction_score(ActualChoice2, Pred8)

t8 <- table(ActualChoice2, PredictChoice8)


accurate_cnt <- c()
for(i in 1:1){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + t8[j,j]
  }
}
accurate_cnt/4550
#without income
#using educincome, wo year 0.5116484
#without educ, year 0.5125275
#without income, year 0.5136264
#without educ, income 0.5162637
#without above and region, 0.5074725
#without all factors, 0.4859341
#with educ+night+ppark+region+segment 0.5134066

```

```{r Pred 8 submission}

model8A <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1 |year+miles+region+segment, data = A2)

summary(model8A)

Pred8A <- predict(model8A, newdata = test2)


#write file
Submission8A <- submission
for (i in 1:nrow(Pred8A)){
  for (j in colnames(Pred8A)){
    Submission8A[i,j] = Pred8A[i,j]
  }
}
write.csv(Submission8A, file = 'Submission7.csv')

```

```{r Pred 9}
model9 <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+HU+Price|miles+region+segment+income, data = A2)

summary(model9)

predict9 <- predict(model9, newdata = test2)


#write file
Submission9 <- submission
for (i in 1:nrow(predict9)){
  for (j in colnames(predict9)){
    Submission9[i,j] = predict9[i,j]
  }
}
write.csv(Submission9, file = 'Submission9.csv')

```

```{r Pred 10}
model10 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+miles+night+age+Urb, data = D2, probit=TRUE)

summary(model10)

predict10 <- predict(model10, newdata = test2)

PredictChoice10 <- apply(predict10, 1, which.max)
t10 <- table(ActualChoice2, PredictChoice10)


accurate_cnt <- c()
for(i in 1:1){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + t10[j,j]
  }
}
accurate_cnt/4550
```


```{r compare accuracy}
accurate_cnt <- c()
for(i in 1:5){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + choiceVector[[i]][j,j]
  }
}
accurate_cnt/4550
```
best prediction
all variables without educ, income, ppark: 51.4%
all variables: 50.989%
3 star variables: 50.791%
2 star variables: 50.989%
1 star variables: 50.83%

```{r pred 7 submission}
#write file
Submission6 <- submission
for (i in 1:nrow(Pred1A)){
  for (j in colnames(Pred1A)){
    Submission6[i,j] = Pred1A[i,j]
  }
}
write.csv(Submission6, file = 'Submission6.csv')

```

```{r model 9}
model9 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price | gender+miles+night+age+Urb , data = D2, rpar = c(CC = 'n',  GN = 'n',  NS = 'n',  BU = 'n',  FA = 'n',  LD = 'n',  BZ = 'n',  FC = 'n',  FP = 'n',  RP = 'n',  PP = 'n',  KA = 'n',  SC = 'n',  TS = 'n',  NV = 'n',  MA = 'n',  LB = 'n',  AF = 'n',  HU = 'n', Price = 'n'), panel = TRUE, print.level = TRUE)
summary(model9)

```

```{r validate 9}
pred9 <- predict(model9, newdata = V2)
choice9 <- apply(pred9, 1, which.max)
t9 <- table(ActualChoice2, choice9)


accurate_cnt <- c()
for(i in 1:1){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + t9[j,j]
  }
}
accurate_cnt/4550
#50.7% accuracy
```

```{r multinomial probit}
library('MNP')
res <- mnp()

predict_nn <- predict(mod, test_nn,"probs")
choice_nn <- apply(predict_nn, 1, which.max)
t_chi <- table(ActualChoice2, choice_nn)


accurate_cnt <- c()
for(i in 1:1){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + t9[j,j]
  }
}
accurate_cnt/4550
```

```{r neural network}

train_nn <- subset(train, Task <= 12)
test_nn <- subset(train, Task > 12)

mod <- multinom(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+miles+night+age+Urb, data=D2)

predict_nn <- predict(mod, V2,"probs")
choice_nn <- apply(predict_nn, 1, which.max)
t_nn <- table(ActualChoice2, choice_nn)


accurate_cnt <- c()
for(i in 1:1){
  accurate_cnt <- c(accurate_cnt, 0)
  for(j in 1:4){
    accurate_cnt[i] <- accurate_cnt[i] + t_nn[j,j]
  }
}
accurate_cnt/4550
```

```{r}
model11 <- mlogit(Choice~GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price -1| gender+age+Urb, data = A2)

predict11 <- predict(model11, newdata = test2)


#write file
Submission11 <- submission
for (i in 1:nrow(predict11)){
  for (j in colnames(predict11)){
    Submission11[i,j] = predict11[i,j]
  }
}
write.csv(Submission11, file = 'Submission11.csv')
```


##CART

```{r cart}
library(rpart)
library(rpart.plot)

train_split =read.csv("output.csv")

cart1 = rpart(Choice~CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,data=train,method = "class") 
cart2 = rpart(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price,data=train_split,method="class") 

prp(cart1)
prp(cart2)
```

```{r cross-validation with six models}

```

##Clustering Models

###Using factors

```{r pre-processing data}

#ClusterTrainData <- train[train$Task == 1,c(84,86:94)]
ClusterTrainData <- train[train$Task == 1,c(88:89,92)]
fac <- factor(c())
levels <- c()
for(i in c('gender', 'age', 'Urb')){
  #print(levels(ClusterTrainData[,i]))
  fac <- union(fac, levels(ClusterTrainData[,i]))
}
start = 1
for(i in colnames(ClusterTrainData)){
  end = start + length(levels(ClusterTrainData[,i])) - 1
  for(j in fac[start:end]){
    #print(c(i,j))
    ClusterTrainData[j] <- ClusterTrainData[,i] == j
  }
  start = end + 1
  ClusterTrainData[,i] <- NULL
}

#ClusterTestData <- test[test$Task == 1,c(84,86:94)]
ClusterTestData <- test[test$Task == 1,c(84,86:89,91:92)]
fac <- factor(c())
levels <- c()
for(i in colnames(ClusterTestData)){
  #print(levels(ClusterTrainData[,i]))
  fac <- union(fac, levels(ClusterTestData[,i]))
}
start = 1
for(i in colnames(ClusterTestData)){
  end = start + length(levels(ClusterTestData[,i])) - 1
  for(j in fac[start:end]){
    #print(c(i,j))
    ClusterTestData[j] <- ClusterTestData[,i] == j
  }
  start = end + 1
  ClusterTestData[,i] <- NULL
}

```

```{r clustering and modelling using kmeans & numerical variables}
actualVector <- vector(mode = "list", length = 6)
trainVector <- vector(mode = "list", length = 6)
validateVector <- vector(mode = "list", length = 6)

scores <- vector(mode="list", length = 10)

for(i in 2:10){
  cluster <- kmeans(ClusterTrainData, centers = i)
  clusterTrain <- train
  clusterTest <- test
  clusterTrain$cluster <- cluster$cluster[clusterTrain$Case]
  clusterTest$cluster <- cluster$cluster[clusterTest$Case]
  models <- vector(mode = "list", length = i)


  
  start <- c(1, 4, 7, 10, 13, 16)
  stop <- c(4, 7, 10, 13, 16, 19)
  score_for_cluster_size = c()  
  score <- c()
  for (j in 1:6){
    trainVector[[j]] <-subset(clusterTrain, Task >= start[j]&Task < stop[j]) 
    validateVector[[j]] <-subset(clusterTrain, !(Task >= start[j]&Task < stop[j]))
    actualVector[[j]] <-subset(clusterTrain, !(Task >= start[j]&Task < stop[j]))[,'Choice']
    
    temp <- validateVector[[j]]
    temp$p1 <- 0
    temp$p2 <- 0
    temp$p3 <- 0
    temp$p4 <- 0
    for(k in 1:i){
      filteredTrain <- trainVector[[j]]
      filteredTrain <- filteredTrain[filteredTrain$cluster == k,]  
      filteredV <- validateVector[[j]]
      filteredV <- filteredV[filteredV$cluster == k,]  
      
      DC <- mlogit.data(filteredTrain, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
        
      VC <- mlogit.data(filteredV, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
      
      model1 <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = DC)
      
      predictmodel1 <- predict(model1, newdata = VC)
      l <- 1
      for (row in temp[temp$cluster == k,]$No){
          temp[temp$No == row,101] <- predictmodel1[l,1]
          temp[temp$No == row,102] <- predictmodel1[l,2]
          temp[temp$No == row,103] <- predictmodel1[l,3]
          temp[temp$No == row,104] <- predictmodel1[l,4]
          l <- l+1
      }
    }
    score <- c(score, prediction_score(actualVector[[j]], temp[,101:104]))
    #print(c('cluster',j,'length',nrow(filteredTrain)))
    #print(score)
  }
  print(score)
}

```

It is established that using five clusters yield the best results during validation.

```{r predicting test case}
closest.cluster <- function(x) {
  cluster.dist <- apply(km$centers, 1, function(y) sqrt(sum((x-y)^2)))
  return(which.min(cluster.dist)[1])
}

km <- kmeans(ClusterTrainData, centers = 4)
clusterTrain$cluster <- km$cluster[clusterTrain$Case]
clusterTest$cluster <- apply(ClusterTestData, 1, closest.cluster)

all_predictions <- data.frame()

for(j in 1:5){
#    print(c('cluster',j))
  filteredTrain <- clusterTrain[clusterTrain$cluster == j,]
  filteredTest <- clusterTest[clusterTest$cluster == j & !is.na(clusterTest$cluster == j),]
  #print(c('cluster',j,'length',nrow(filteredTrain)))
  
  AC <- mlogit.data(filteredTrain, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
  
  TestC <- mlogit.data(filteredTest, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
  TestC$Ch1 = 0
  TestC$Ch2 = 0
  TestC$Ch3 = 0
  TestC$Ch4 = 0
  TestC$Choice = FALSE

  modelC <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = AC)
  index <- filteredTest$No
  predictC <- data.frame(predict(modelC, newdata = TestC))
  predictC$index <- index
  all_predictions <- rbind(all_predictions, predictC)
}

#Using base model for nulls

filteredTest <- clusterTest[is.na(clusterTest$cluster == j),]
TestC <- mlogit.data(filteredTest, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
TestC$Ch1 = 0
TestC$Ch2 = 0
TestC$Ch3 = 0
TestC$Ch4 = 0
TestC$Choice = FALSE
index <- filteredTest$No
predictC <- data.frame(predict(model1A, newdata = TestC))
predictC$index <- index
all_predictions <- rbind(all_predictions, predictC)

Submission8 <- submission
for (i in 1:nrow(Submission8)){
  index <- Submission8[i,]$No
  for (j in colnames(TestPredict)){
    Submission8[i,j] = all_predictions[all_predictions$index == index,][,j]
  }
}
write.csv(Submission8, file = 'Submission8.csv')

```

###Using numbers

```{r pre-processing data}
ClusterTrainData <- train2[train2$Task == 1,c(88:89,92)]
fac <- factor(c())
for(i in c('gender', 'age', 'Urb')){
  #print(levels(ClusterTrainData[,i]))
  fac <- union(levels(ClusterTrainData[,i]), fac)
}
for(i in fac[1:5]){
  ClusterTrainData[i] <- ClusterTrainData$region == i
}
for(i in fac[5:11]){
  ClusterTrainData[i] <- ClusterTrainData$segment == i
}
ClusterTrainData$region <- NULL
ClusterTrainData$segment <- NULL
#fac <- factor(c())
#for(i in colnames(ClusterTrainData)){
#  #print(levels(ClusterTrainData[,i]))
#  fac <- union(levels(ClusterTrainData[,i]), fac)
#}
clusterVector <- vector(mode = "list", length = 9)
```

```{r clustering and modelling using kmeans & numerical variables}
for(i in 1:10){
  cluster <- kmeans(ClusterTrainData, centers = i)
  clusterTrain <- train2
  clusterTest <- test2
  clusterTrain$cluster <- cluster$cluster[clusterTrain$Case]
  clusterTest$cluster <- cluster$cluster[clusterTest$Case]
  models <- vector(mode = "list", length = i)
  total_correct = 0
  print(i)
  for(j in 1:i){
#    print(c('cluster',j))
    filteredTrain <- clusterTrain[clusterTrain$cluster == j,]
    #print(c('cluster',j,'length',nrow(filteredTrain)))
    
    AC <- mlogit.data(filteredTrain, shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
    
    DC <- mlogit.data(subset(filteredTrain, Task <= 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
    
    VC <- mlogit.data(subset(filteredTrain, Task > 12), shape = 'wide', choice = "Choice", varying = c(4:83), sep = '', alt.levels = c("Ch1", "Ch2", "Ch3", "Ch4"), id.var = "Case")
    
    ActualChoiceC <- subset(filteredTrain, Task > 12)[,"Choice"]
    
    modelC <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = DC)
    
    predictC <- predict(modelC, newdata = VC)
    PredictChoiceC <- apply(predictC, 1, which.max)
    t <- table(PredictChoiceC, ActualChoiceC)
    #print(t)
    correct <- 0
    for (choice in 1:4){
      correct = correct + t[choice,choice]
    }
    total_correct = total_correct + correct
    #print(total_correct)
  }
  print(total_correct)
}

```

We establish that k = 2 is best but loses to baseline model


